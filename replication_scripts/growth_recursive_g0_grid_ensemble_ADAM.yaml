program: growth_recursive.py
name: growth_recursive_g0_grid_ensemble_ADAM
description: Ensemble run with growth recursive with g = 0 (100 seeds) on the grid using ADAM 
method: grid
parameters:
  trainer.logger.tags:
    value: [growth_recursive_g0_grid_ensemble_ADAM]
  seed:
    min: 1
    max: 100
  trainer.logger.offline:
    value: false # log online for W&B optimization
  trainer.max_time:
    value: 00:00:10:00  # these tests shouldn't take more than 2-3 minutes
   # Stopping criteria
  trainer.callbacks.monitor:
    value: train_loss
  trainer.callbacks.stopping_threshold:
    value: 1.0e-7
  trainer.max_epochs: 
    value: 2000
  trainer.min_epochs: 
    value: 20
  model.max_T_test:
    value: 0

#ADAM optim values 
  optimizer.class_path:
    value: torch.optim.Adam
  optimizer.lr: 
      value: 0.001

  lr_scheduler.class_path:
    value: torch.optim.lr_scheduler.StepLR
  lr_scheduler.step_size: 
    value: 100
  lr_scheduler.gamma:
      value: 0.9


  