program: growth_seq.py
project: TVC_examples
name: train_loss_growth_seq_g_positive
description: HPO for train_loss growth sequential g=0.02 contiguous grid
method: bayes
metric:
  # See the  growth_seq.py log_and_save
  name: hpo_objective 
  goal: minimize

parameters:
  # Fix parameters here:
  trainer.logger.offline:
    value: false # log online for W&B optimization
  trainer.max_time:
    value: 00:00:10:00  # these tests shouldn't take more than 5 minutes, I increased a time as it is ADAM
  trainer.max_epochs: 
    value: 2000


  model.hpo_objective_name:
    value: train_loss
  model.always_log_hpo_objective:
    value: true
   # Stopping criteria
  trainer.callbacks.monitor:
    value: train_loss
  trainer.callbacks.stopping_threshold:
    value: 1.0e-7

# Economic model arameters 
  model.g:
    value: 0.02
# Provide distributions and variations for optimizer
  optimizer.class_path:
    value: torch.optim.Adam

  optimizer.lr: #higher lr always crash
      min: 0.00001
      max: 0.01

  lr_scheduler.class_path:
    value: torch.optim.lr_scheduler.StepLR
  lr_scheduler.step_size: #I added additional traning on this 
      min: 80
      max: 120
  lr_scheduler.gamma:
      min: 0.8
      max: 0.99

#add rescaling for positive g case
  model.ml_model.class_path:
    value:  econ_layers.layers.FlexibleSequential
  model.ml_model.init_args.n_in:
    value: 1
  model.ml_model.init_args.n_out:
    value: 1
  model.ml_model.init_args.layers:
    value: 4
  model.ml_model.init_args.hidden_dim: 
    value: 128
  model.ml_model.init_args.hidden_bias: 
    value: true
  model.ml_model.init_args.last_bias: 
    value: true
  model.ml_model.init_args.activator.class_path:
    value: torch.nn.Tanh
  model.ml_model.init_args.last_activator.class_path:
    value: torch.nn.Softplus
  model.ml_model.init_args.OutputRescalingLayer.class_path: 
    value: econ_layers.layers.ScalarExponentialRescaling
